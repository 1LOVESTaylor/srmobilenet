{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1 or  classname.find('InstanceNorm2d') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def get_norm_layer(norm_type):\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = nn.InstanceNorm2d\n",
    "    else:\n",
    "        print('normalization layer [%s] is not found' % norm)\n",
    "    return norm_layer\n",
    "\n",
    "# Defines the Unet generator.\n",
    "# |num_downs|: number of downsamplings in UNet. For example,\n",
    "# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n",
    "# at the bottleneck\n",
    "class UnetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n",
    "                 norm_layer=nn.BatchNorm2d, use_dropout=False, gpu_ids=[]):\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        self.gpu_ids = gpu_ids\n",
    "\n",
    "        # currently support only input_nc == output_nc\n",
    "        assert(input_nc == output_nc)\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, innermost=True)\n",
    "        for i in range(num_downs - 5):\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(output_nc, ngf, unet_block, outermost=True, norm_layer=norm_layer)\n",
    "\n",
    "        self.model = unet_block\n",
    "\n",
    "    def forward(self, input):\n",
    "        # embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # embedded.size\n",
    "        if  self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "        else:\n",
    "            return self.model(input)\n",
    "\n",
    "\n",
    "# Defines the submodule with skip connection.\n",
    "# X -------------------identity---------------------- X\n",
    "#   |-- downsampling -- |submodule| -- upsampling --|\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    def __init__(self, outer_nc, inner_nc,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "\n",
    "        downconv = nn.Conv2d(outer_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc, affine=True)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc, affine=True)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([self.model(x), x], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defines the Unet+MobileNet generator.\n",
    "\n",
    "class UnetMobileNetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n",
    "                 norm_layer=nn.BatchNorm2d, use_dropout=False, gpu_ids=[]):\n",
    "        super(UnetMobileNetGenerator, self).__init__()\n",
    "        self.gpu_ids = gpu_ids\n",
    "\n",
    "        # currently support only input_nc == output_nc\n",
    "        assert(input_nc == output_nc)\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UnetMobileNetSkipConnectionBlock(ngf * 8, ngf * 8, innermost=True)\n",
    "        for i in range(num_downs - 5):\n",
    "            unet_block = UnetMobileNetSkipConnectionBlock(ngf * 8, ngf * 8, unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        unet_block = UnetMobileNetSkipConnectionBlock(ngf * 4, ngf * 8, unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetMobileNetSkipConnectionBlock(ngf * 2, ngf * 4, unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetMobileNetSkipConnectionBlock(ngf, ngf * 2, unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetMobileNetSkipConnectionBlock(output_nc, ngf, unet_block, outermost=True, norm_layer=norm_layer)\n",
    "\n",
    "        self.model = unet_block\n",
    "\n",
    "    def forward(self, input):\n",
    "        # embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # embedded.size\n",
    "        if  self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "        else:\n",
    "            return self.model(input)\n",
    "\n",
    "# Defines the submodule with skip connection.\n",
    "class UnetMobileNetSkipConnectionBlock(nn.Module):\n",
    "    def __init__(self, outer_nc, inner_nc,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UnetMobileNetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "\n",
    "        #downconv = nn.Conv2d(outer_nc, inner_nc, kernel_size=4,\n",
    "        #                     stride=2, padding=1)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc, affine=True)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc, affine=True)\n",
    "\n",
    "        if outermost: # Equal to call conv_bn\n",
    "            downconv = nn.Conv2d(outer_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=True)\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv, downnorm] # different from original setup\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost: # Equal to call conv_dw without submodule            \n",
    "            #downconv_d = nn.Conv2d(outer_nc, outer_nc, kernel_size=4,\n",
    "            #                 stride=2, padding=1)\n",
    "            downconv_d = nn.Conv2d(outer_nc, outer_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, groups=outer_nc, bias=True)\n",
    "            downconv_s = nn.Conv2d(outer_nc, inner_nc, kernel_size=1,\n",
    "                             stride=1, padding=0, bias=True)\n",
    "            \n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            upconv_d = nn.ConvTranspose2d(inner_nc, inner_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, groups=inner_nc)\n",
    "            upconv_s = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=1, stride=1,\n",
    "                                        padding=0)\n",
    "            upnorm_d = norm_layer(inner_nc, affine=True)\n",
    "            \n",
    "            #down = [downrelu, downconv_d, downnorm, downrelu, downconv_s, downnorm]\n",
    "            down = [downrelu, downconv_d, downnorm, downrelu, downconv_s, downnorm]\n",
    "            up = [uprelu, upconv_d, upnorm_d, uprelu, upconv_s, upnorm]\n",
    "            #up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else: # Equal to call conv_dw            \n",
    "            downconv_d = nn.Conv2d(outer_nc, outer_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, groups=outer_nc, bias=True)\n",
    "            downconv_s = nn.Conv2d(outer_nc, inner_nc, kernel_size=1,\n",
    "                             stride=1, padding=0, bias=True)\n",
    "            downnorm_d = norm_layer(outer_nc, affine=True)\n",
    "            \n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            \n",
    "            \n",
    "            upconv_d = nn.ConvTranspose2d(inner_nc * 2, inner_nc * 2,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, groups=inner_nc * 2)\n",
    "            upconv_s = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=1, stride=1,\n",
    "                                        padding=0)\n",
    "            upnorm_d = norm_layer(inner_nc * 2, affine=True)\n",
    "            \n",
    "            down = [downrelu, downconv_d, downnorm_d, downrelu, downconv_s, downnorm]\n",
    "            up = [uprelu, upconv_d, upnorm_d, uprelu, upconv_s, upnorm]\n",
    "            #up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def conv_bn(inp, oup, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def conv_dw(inp, oup, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "            nn.BatchNorm2d(inp),\n",
    "            nn.ReLU(inplace=True),\n",
    "    \n",
    "            nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([self.model(x), x], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the Unet+MobileNet V2 generator.\n",
    "\n",
    "class UnetMobileNet2Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n",
    "                 norm_layer=nn.BatchNorm2d, use_dropout=False, gpu_ids=[]):\n",
    "        super(UnetMobileNet2Generator, self).__init__()\n",
    "        self.gpu_ids = gpu_ids\n",
    "\n",
    "        # currently support only input_nc == output_nc\n",
    "        assert(input_nc == output_nc)\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UnetMobileNet2SkipConnectionBlock(ngf * 8, ngf * 8, innermost=True)\n",
    "        \n",
    "        #for i in range(num_downs - 5):\n",
    "        #    unet_block = UnetMobileNet2SkipConnectionBlock(ngf * 8, ngf * 8, unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        #unet_block = UnetMobileNet2SkipConnectionBlock(ngf * 4, ngf * 8, unet_block, norm_layer=norm_layer)\n",
    "        #unet_block = UnetMobileNet2SkipConnectionBlock(ngf * 2, ngf * 4, unet_block, norm_layer=norm_layer)\n",
    "        #unet_block = UnetMobileNet2SkipConnectionBlock(ngf, ngf * 2, unet_block, norm_layer=norm_layer)\n",
    "        #unet_block = UnetMobileNet2SkipConnectionBlock(output_nc, ngf, unet_block, outermost=True, norm_layer=norm_layer)\n",
    "\n",
    "        self.model = unet_block\n",
    "\n",
    "    def forward(self, input):\n",
    "        # embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # embedded.size\n",
    "        if  self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "        else:\n",
    "            return self.model(input)\n",
    "\n",
    "# Defines the submodule with skip connection.\n",
    "class UnetMobileNet2SkipConnectionBlock(nn.Module):\n",
    "    def __init__(self, outer_nc, inner_nc,\n",
    "                 submodule=None, outermost=False, innermost=False, expand_ratio=6, \n",
    "                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UnetMobileNet2SkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "\n",
    "        downrelu = nn.ReLU6(inplace=True)\n",
    "        downnorm = norm_layer(inner_nc, affine=True)\n",
    "        uprelu = nn.ReLU6(inplace=True)\n",
    "        upnorm = norm_layer(outer_nc, affine=True)\n",
    "\n",
    "        if outermost: # Equal to call conv_bn\n",
    "            downconv = nn.Conv2d(outer_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=True)\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv, downnorm] # different from original setup\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] # + up\n",
    "        elif innermost: # Equal to call InvertedResidual without submodule            \n",
    "            \n",
    "            # pw\n",
    "            downconv_p = nn.Conv2d(outer_nc, outer_nc*expand_ratio, kernel_size=1,\n",
    "                                  stride=1, padding=0, bias=False)\n",
    "            downnorm_p = nn.BatchNorm2d(outer_nc*expand_ratio)\n",
    "            # dw\n",
    "            downconv_d = nn.Conv2d(outer_nc*expand_ratio, outer_nc*expand_ratio, kernel_size=4,\n",
    "                                  stride=2, padding=1, groups=outer_nc*expand_ratio, bias=False)\n",
    "            downnorm_d = nn.BatchNorm2d(outer_nc*expand_ratio)\n",
    "            # pw-linear\n",
    "            downconv_l = nn.Conv2d(outer_nc*expand_ratio, inner_nc, kernel_size=1,\n",
    "                                  stride=1, padding=0, bias=False)\n",
    "            downnorm_l = nn.BatchNorm2d(inner_nc)\n",
    "            \n",
    "            # pw\n",
    "            upconv_p = nn.ConvTranspose2d(inner_nc, inner_nc*expand_ratio,\n",
    "                                        kernel_size=1, stride=1,\n",
    "                                        padding=0, bias=False)\n",
    "            upnorm_p = nn.BatchNorm2d(inner_nc*expand_ratio)\n",
    "            # dw\n",
    "            upconv_d = nn.ConvTranspose2d(inner_nc*expand_ratio, inner_nc*expand_ratio,\n",
    "                                         kernel_size=4, stride=2,\n",
    "                                         padding=1, groups=inner_nc*expand_ratio, bias=False)\n",
    "            upnorm_d = nn.BatchNorm2d(inner_nc*expand_ratio)\n",
    "            # pw-linear\n",
    "            upconv_l = nn.ConvTranspose2d(inner_nc*expand_ratio, outer_nc,\n",
    "                                         kernel_size=1, stride=1,\n",
    "                                         padding=0, bias=False)\n",
    "            upnorm_l = nn.BatchNorm2d(outer_nc) # Normalization can be switched to the norm_layer\n",
    "            \n",
    "            down = [downrelu, downconv_p, downnorm_p, downrelu, downconv_d, downnorm_d,\n",
    "                    downrelu, downconv_l, downnorm_l]\n",
    "            up = [uprelu, upconv_p, upnorm_p, uprelu, upconv_d, upnorm_d, \n",
    "                  uprelu, upconv_l, upnorm_l]\n",
    "            model = down # + up\n",
    "        else: # Equal to call InvertedResidual\n",
    "            \n",
    "            # pw\n",
    "            downconv_p = nn.Conv2d(outer_nc, outer_nc*expand_ratio, kernel_size=1,\n",
    "                             stride=1, padding=0, bias=False)\n",
    "            downnorm_p = nn.BatchNorm2d(outer_nc*expand_ratio)\n",
    "            # dw\n",
    "            downconv_d = nn.Conv2d(outer_nc*expand_ratio, outer_nc*expand_ratio, kernel_size=4,\n",
    "                                   stride=2, padding=1, groups=outer_nc*expand_ratio, bias=False)\n",
    "            downnorm_d = nn.BatchNorm2d(outer_nc*expand_ratio)\n",
    "            # pw-linear\n",
    "            downconv_l = nn.Conv2d(outer_nc*expand_ratio, inner_nc, kernel_size=1,\n",
    "                             stride=1, padding=0, bias=False)\n",
    "            downnorm_l = nn.BatchNorm2d(inner_nc)\n",
    "            \n",
    "            # pw\n",
    "            upconv_p = nn.ConvTranspose2d(inner_nc*2, inner_nc*2*expand_ratio, kernel_size=1,\n",
    "                                         stride=1, padding=0, bias=False)\n",
    "            upnorm_p = nn.BatchNorm2d(inner_nc*2*expand_ratio)\n",
    "            # dw\n",
    "            upconv_d = nn.ConvTranspose2d(inner_nc*2*expand_ratio, inner_nc*2*expand_ratio, kernel_size=4,\n",
    "                                          stride=2, padding=1, groups=inner_nc*2*expand_ratio, bias=False)\n",
    "            upnorm_d = nn.BatchNorm2d(inner_nc*2*expand_ratio)\n",
    "            # pw-linear\n",
    "            upconv_l = nn.ConvTranspose2d(inner_nc*2*expand_ratio, outer_nc, kernel_size=1,\n",
    "                                         stride=1, padding=0, bias=False)\n",
    "            upnorm_l = nn.BatchNorm2d(outer_nc)\n",
    "            \n",
    "            down = [downrelu, downconv_p, downnorm_p, downrelu, downconv_d, downnorm_d, \n",
    "                    downrelu, downconv_l, downnorm_l]\n",
    "            up = [uprelu, upconv_p, upnorm_p, uprelu, upconv_d, upnorm_d,\n",
    "                  uprelu, upconv_l, upnorm_l]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] # + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] # + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([self.model(x), x], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_G = define_G(3, 3, 64, which_model_netG='unet_256')\n",
    "def define_G(input_nc, output_nc, ngf, which_model_netG, norm='batch', use_dropout=False, gpu_ids=[]):\n",
    "    netG = None\n",
    "    use_gpu = len(gpu_ids) > 0\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if use_gpu:\n",
    "        assert(torch.cuda.is_available())\n",
    "\n",
    "    if which_model_netG == 'resnet_9blocks':\n",
    "        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9, gpu_ids=gpu_ids)\n",
    "    elif which_model_netG == 'resnet_6blocks':\n",
    "        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6, gpu_ids=gpu_ids)\n",
    "    elif which_model_netG == 'unet_128':\n",
    "        netG = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids)\n",
    "    elif which_model_netG == 'unet_256':\n",
    "        netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids)\n",
    "    elif which_model_netG == 'unet_mobilenet_256':\n",
    "        netG = UnetMobileNetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids)\n",
    "    elif which_model_netG == 'unet_mobilenet2_256':\n",
    "        netG = UnetMobileNet2Generator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids)\n",
    "    else:\n",
    "        print('Generator model name [%s] is not recognized' % which_model_netG)\n",
    "    if len(gpu_ids) > 0:\n",
    "        netG.cuda(device_id=gpu_ids[0])\n",
    "    netG.apply(weights_init)\n",
    "    return netG\n",
    "\n",
    "\n",
    "def define_D(input_nc, ndf, which_model_netD,\n",
    "             n_layers_D=3, norm='batch', use_sigmoid=False, gpu_ids=[]):\n",
    "    netD = None\n",
    "    use_gpu = len(gpu_ids) > 0\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if use_gpu:\n",
    "        assert(torch.cuda.is_available())\n",
    "    if which_model_netD == 'basic':\n",
    "        netD = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n",
    "    elif which_model_netD == 'n_layers':\n",
    "        netD = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n",
    "    else:\n",
    "        print('Discriminator model name [%s] is not recognized' %\n",
    "              which_model_netD)\n",
    "    if use_gpu:\n",
    "        netD.cuda(device_id=gpu_ids[0])\n",
    "    netD.apply(weights_init)\n",
    "    return netD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defines the GAN loss which uses either LSGAN or the regular GAN.\n",
    "# When LSGAN is used, it is basically same as MSELoss,\n",
    "# but it abstracts away the need to create the target label tensor\n",
    "# that has the same size as the input\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n",
    "                 tensor=torch.FloatTensor):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.real_label = target_real_label\n",
    "        self.fake_label = target_fake_label\n",
    "        self.real_label_var = None\n",
    "        self.fake_label_var = None\n",
    "        self.Tensor = tensor\n",
    "        if use_lsgan:\n",
    "            self.loss = nn.MSELoss()\n",
    "        else:\n",
    "            self.loss = nn.BCELoss()\n",
    "\n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        target_tensor = None\n",
    "        if target_is_real:\n",
    "            create_label = ((self.real_label_var is None) or\n",
    "                            (self.real_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n",
    "                self.real_label_var = Variable(real_tensor, requires_grad=False)\n",
    "            target_tensor = self.real_label_var\n",
    "        else:\n",
    "            create_label = ((self.fake_label_var is None) or\n",
    "                            (self.fake_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n",
    "                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n",
    "            target_tensor = self.fake_label_var\n",
    "        return target_tensor\n",
    "\n",
    "    def __call__(self, input, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "        return self.loss(input, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_G = define_G(3, 3, 64, which_model_netG='unet_256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobile_model_G = define_G(3, 3, 64, which_model_netG='unet_mobilenet_256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnetGenerator(\n",
      "  (model): UnetSkipConnectionBlock(\n",
      "    (model): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): UnetSkipConnectionBlock(\n",
      "        (model): Sequential(\n",
      "          (0): LeakyReLU(0.2, inplace)\n",
      "          (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (3): UnetSkipConnectionBlock(\n",
      "            (model): Sequential(\n",
      "              (0): LeakyReLU(0.2, inplace)\n",
      "              (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "              (3): UnetSkipConnectionBlock(\n",
      "                (model): Sequential(\n",
      "                  (0): LeakyReLU(0.2, inplace)\n",
      "                  (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                  (3): UnetSkipConnectionBlock(\n",
      "                    (model): Sequential(\n",
      "                      (0): LeakyReLU(0.2, inplace)\n",
      "                      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                      (3): UnetSkipConnectionBlock(\n",
      "                        (model): Sequential(\n",
      "                          (0): LeakyReLU(0.2, inplace)\n",
      "                          (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                          (3): UnetSkipConnectionBlock(\n",
      "                            (model): Sequential(\n",
      "                              (0): LeakyReLU(0.2, inplace)\n",
      "                              (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                              (3): UnetSkipConnectionBlock(\n",
      "                                (model): Sequential(\n",
      "                                  (0): LeakyReLU(0.2, inplace)\n",
      "                                  (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                                  (2): ReLU(inplace)\n",
      "                                  (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                                  (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                                )\n",
      "                              )\n",
      "                              (4): ReLU(inplace)\n",
      "                              (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                              (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                            )\n",
      "                          )\n",
      "                          (4): ReLU(inplace)\n",
      "                          (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                          (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                        )\n",
      "                      )\n",
      "                      (4): ReLU(inplace)\n",
      "                      (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (4): ReLU(inplace)\n",
      "                  (5): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "                )\n",
      "              )\n",
      "              (4): ReLU(inplace)\n",
      "              (5): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "              (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "            )\n",
      "          )\n",
      "          (4): ReLU(inplace)\n",
      "          (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "          (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): ReLU(inplace)\n",
      "      (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (4): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 54419459\n"
     ]
    }
   ],
   "source": [
    "print_network(model_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnetMobileNetGenerator(\n",
      "  (model): UnetMobileNetSkipConnectionBlock(\n",
      "    (model): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (2): UnetMobileNetSkipConnectionBlock(\n",
      "        (model): Sequential(\n",
      "          (0): LeakyReLU(0.2, inplace)\n",
      "          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=64)\n",
      "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (3): LeakyReLU(0.2, inplace)\n",
      "          (4): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (6): UnetMobileNetSkipConnectionBlock(\n",
      "            (model): Sequential(\n",
      "              (0): LeakyReLU(0.2, inplace)\n",
      "              (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=128)\n",
      "              (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "              (3): LeakyReLU(0.2, inplace)\n",
      "              (4): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "              (6): UnetMobileNetSkipConnectionBlock(\n",
      "                (model): Sequential(\n",
      "                  (0): LeakyReLU(0.2, inplace)\n",
      "                  (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "                  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "                  (3): LeakyReLU(0.2, inplace)\n",
      "                  (4): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                  (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                  (6): UnetMobileNetSkipConnectionBlock(\n",
      "                    (model): Sequential(\n",
      "                      (0): LeakyReLU(0.2, inplace)\n",
      "                      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=512)\n",
      "                      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                      (3): LeakyReLU(0.2, inplace)\n",
      "                      (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                      (6): UnetMobileNetSkipConnectionBlock(\n",
      "                        (model): Sequential(\n",
      "                          (0): LeakyReLU(0.2, inplace)\n",
      "                          (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=512)\n",
      "                          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                          (3): LeakyReLU(0.2, inplace)\n",
      "                          (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                          (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                          (6): UnetMobileNetSkipConnectionBlock(\n",
      "                            (model): Sequential(\n",
      "                              (0): LeakyReLU(0.2, inplace)\n",
      "                              (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=512)\n",
      "                              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                              (3): LeakyReLU(0.2, inplace)\n",
      "                              (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                              (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                              (6): UnetMobileNetSkipConnectionBlock(\n",
      "                                (model): Sequential(\n",
      "                                  (0): LeakyReLU(0.2, inplace)\n",
      "                                  (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=512)\n",
      "                                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                                  (3): LeakyReLU(0.2, inplace)\n",
      "                                  (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                                  (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                                  (6): ReLU(inplace)\n",
      "                                  (7): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=512)\n",
      "                                  (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                                  (9): ReLU(inplace)\n",
      "                                  (10): ConvTranspose2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                                  (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                                )\n",
      "                              )\n",
      "                              (7): ReLU(inplace)\n",
      "                              (8): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=1024)\n",
      "                              (9): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "                              (10): ReLU(inplace)\n",
      "                              (11): ConvTranspose2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                              (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                            )\n",
      "                          )\n",
      "                          (7): ReLU(inplace)\n",
      "                          (8): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=1024)\n",
      "                          (9): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "                          (10): ReLU(inplace)\n",
      "                          (11): ConvTranspose2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                          (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                        )\n",
      "                      )\n",
      "                      (7): ReLU(inplace)\n",
      "                      (8): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=1024)\n",
      "                      (9): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "                      (10): ReLU(inplace)\n",
      "                      (11): ConvTranspose2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                      (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (7): ReLU(inplace)\n",
      "                  (8): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=1024)\n",
      "                  (9): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "                  (10): ReLU(inplace)\n",
      "                  (11): ConvTranspose2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "                )\n",
      "              )\n",
      "              (7): ReLU(inplace)\n",
      "              (8): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=512)\n",
      "              (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "              (10): ReLU(inplace)\n",
      "              (11): ConvTranspose2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "            )\n",
      "          )\n",
      "          (7): ReLU(inplace)\n",
      "          (8): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (10): ReLU(inplace)\n",
      "          (11): ConvTranspose2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): ReLU(inplace)\n",
      "      (4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (5): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 3573955\n"
     ]
    }
   ],
   "source": [
    "print_network(mobile_model_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile2_model_G = define_G(3, 3, 64, which_model_netG='unet_mobilenet2_256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnetMobileNet2Generator(\n",
      "  (model): UnetMobileNet2SkipConnectionBlock(\n",
      "    (model): Sequential(\n",
      "      (0): ReLU6(inplace)\n",
      "      (1): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (3): ReLU6(inplace)\n",
      "      (4): Conv2d(3072, 3072, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=3072, bias=False)\n",
      "      (5): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (6): ReLU6(inplace)\n",
      "      (7): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 3208192\n"
     ]
    }
   ],
   "source": [
    "print_network(mobile2_model_G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
